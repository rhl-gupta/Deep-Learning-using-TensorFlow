{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define two constants 'a' & 'b'; since they are constants, we need to assign values while defining them \n",
    "a = tf.constant(10,name='a')\n",
    "b = tf.constant(2,name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a computation between constants defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a multiplication operation between the constants defined above, for that we need to define computations/formula/logic\n",
    "result = tf.Variable((a*b), name='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilaizing the variable defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a session to excute the above computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the session created above, execute the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Before excuting the computations, need to initialize the variables defined above i.e. result variable needs to initialized\n",
    "sess.run(init)\n",
    "\n",
    "# Running the session to execute the comutations & print results\n",
    "print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the same above multiplication but using variables (not fixed constants) to which we assign value during run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Let's use the same name (a & b) again for the operands for easy understanding\n",
    "a = tf.placeholder(tf.int64, name='a')\n",
    "b = tf.placeholder(tf.int64, name='b')\n",
    "\n",
    "# Defining the computations (multiplication in this case) using the opernads a & b defined above\n",
    "result = a*b\n",
    "\n",
    "#Creating a session and excute the computations in that session\n",
    "sess = tf.Session()\n",
    "print(sess.run(result,feed_dict = {a:10,b:2}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex compuations (WX+b) using placeholders, it is used in designing NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function which computes WX+b, when we supply X\n",
    "\n",
    "def complex_computation(matrix_supplied):\n",
    "    X=matrix_supplied\n",
    "    W= np.random.randn(np.shape(X)[1],np.shape(X)[0])\n",
    "    b=np.random.randn(np.shape(X)[1],1)\n",
    "    result = tf.add(tf.matmul(W,matrix_supplied),b)\n",
    "                       \n",
    "    sess = tf.Session()\n",
    "    result = sess.run(result)\n",
    "    sess.close()\n",
    "    \n",
    "    return result\n",
    "            \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12490719,  1.13135168],\n",
       "       [-0.31910889, -0.70716677]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.rand(3,2)\n",
    "\n",
    "r = complex_computation(z)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we have a classification problem with 5 classes/categories. In deep learnig these are converted into a matrix form\n",
    "classes = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.one_hot(classes,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating cost, which is the last step in forward propagation in NN design\n",
    "\n",
    "Using Softmax as activation function and cross entropy as cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define Z = WX + b and estimated Y i.e Yhat\n",
    "\n",
    "Z = [10.,9.,8.,3.,5.]\n",
    "Y = [11.,10.,8.,5.,1.]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost = 80.44387\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"Cost = \" + str(sess.run(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
